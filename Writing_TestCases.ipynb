{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Notebook to create test cases for the problem identified in the step 1 - Classifying user mobile behaviour.**\n"
      ],
      "metadata": {
        "id": "U3Uy5GWTp69O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_VdUempnp6y-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "rlQdAy-9S0KQ"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from openai import AsyncAzureOpenAI\n",
        "\n",
        "import asyncio\n",
        "from openai import AsyncAzureOpenAI\n",
        "\n",
        "\n",
        "azure_oai_endpoint = \"\"\n",
        "azure_oai_key = \"\"\n",
        "azure_oai_deployment = \"\"\n",
        "\n",
        "# Configure the Azure OpenAI client\n",
        "client = AsyncAzureOpenAI(\n",
        "    azure_endpoint =azure_oai_endpoint ,\n",
        "    api_key=azure_oai_key,\n",
        "    api_version=\"2024-02-15-preview\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dWhO4bh2p5Kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# System prompt for setting the context and role of the AI code expert\n",
        "\n",
        "system_message = '''You are a helpful AI assistant that helps programmers write code.\n",
        "                    You are expert at analyzing the code, input and generated ouput. You are\n",
        "                    provided with a specific ask related to code in python language, you carefully assess the\n",
        "                    input query and code and generate the response in the python language.\n",
        "                    '''\n"
      ],
      "metadata": {
        "id": "P9BfERIIeUFS"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# writing test cases for the python code used in the ML notebook problem 1- Identifying user mobile behaviour\n",
        "user_message = '''Write unit test cases and add comments to the python code provided in the input. Return the response in the python code format.\n",
        "                '''\n",
        "\n",
        "file = open(file=\"function.py\", encoding=\"utf8\").read()\n",
        "user_message= user_message + file\n",
        "# Format and send the request to the model\n",
        "messages =[\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_message}\n",
        "\n",
        "]\n",
        "# Call the Azure OpenAI model\n",
        "response = await client.chat.completions.create(\n",
        "    model=azure_oai_deployment,\n",
        "    messages=messages,\n",
        "    temperature=0.7,\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "# Print the response to the console, if desired\n",
        "print(response)\n",
        "\n",
        "# Write the response to a file\n",
        "results_file = open(file=\"unittestcases_commented.txt\", mode=\"w\", encoding=\"utf8\")\n",
        "results_file.write(response.choices[0].message.content)\n",
        "print(\"\\nResponse written to result/file.txt\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNLfYGCFee6i",
        "outputId": "93ca6dd3-ebe5-4c54-bbf9-4fdd59f22108"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-AYtARlEFtAcwWCedEx7eGiRYh3pFz', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='# Unit Test Cases and Comments\\n\\n# Import necessary libraries\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import SVC\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.model_selection import train_test_split, cross_val_score\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\\nfrom sklearn.model_selection import RandomizedSearchCV\\n\\n# Separate features and target variables\\nX = df.drop(columns=[\"user_behavior_class\"], axis=1)\\ny = df[\"user_behavior_class\"]\\n\\n# Get categorical and numerical columns\\ncat_cols = X.select_dtypes(include=[\\'object\\']).columns.values\\nnum_cols = X.select_dtypes(include=np.number).columns.tolist()\\n\\n# Data Preprocessing\\npreprocessor = ColumnTransformer(\\n    transformers=[\\n        (\\'num\\', StandardScaler(), num_cols),\\n        (\\'cat\\', OneHotEncoder(drop=\\'first\\'), cat_cols)\\n    ])\\n\\n# Split the data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, random_state=42, stratify=y)\\n\\n# Initialize the classifier\\nclf = LogisticRegression(max_iter=200)\\n\\n# Initialize scores dictionary to store evaluation metrics\\nscores = {\\'accuracy\\': {}, \\'precision\\': {}, \\'recall\\': {}, \\'f1-score\\': {}}\\n\\n# Create a pipeline for data preprocessing and classification\\nmodel = Pipeline(\\n    steps=[\\n        (\\'preprocessor\\', preprocessor),\\n        (\\'classifier\\', clf)\\n    ])\\n\\n# Fit the model on the training data\\nmodel.fit(X_train, y_train)\\n\\n# Make predictions on the test data\\ny_pred = model.predict(X_test)\\n\\n# Generate classification report\\nreport = classification_report(y_test, y_pred, output_dict=True)\\n\\n# Calculate and store evaluation metrics\\nscores[\\'accuracy\\'][name] = accuracy_score(y_test, y_pred)\\nscores[\\'precision\\'][name] = report[\\'macro avg\\'][\\'precision\\']\\nscores[\\'recall\\'][name] = report[\\'macro avg\\'][\\'recall\\']\\nscores[\\'f1-score\\'][name] = report[\\'macro avg\\'][\\'f1-score\\']', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1732878579, model='gpt-35-turbo-16k', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=468, prompt_tokens=494, total_tokens=962, completion_tokens_details=None, prompt_tokens_details=None), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n",
            "\n",
            "Response written to result/file.txt\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing validation test for data for problem1- Identifying user's mobile behaviour\n",
        "\n",
        "user_message= ''' Write the validation unit test cases in python code for the data based on the description provided.\n",
        "\n",
        "This dataset provides a comprehensive analysis of mobile device usage patterns and user behavior classification. It contains 700 samples of user data, including metrics such as app usage time, screen-on time, battery drain, and data consumption. Each entry is categorized into one of five user behavior classes, ranging from light to extreme usage, allowing for insightful analysis and modeling.\n",
        "\n",
        "Key Features:\n",
        "\n",
        "User ID: Unique identifier for each user.\n",
        "Device Model: Model of the user's smartphone.\n",
        "Operating System: The OS of the device (iOS or Android).\n",
        "App Usage Time: Daily time spent on mobile applications, measured in minutes.\n",
        "Screen On Time: Average hours per day the screen is active.\n",
        "Battery Drain: Daily battery consumption in mAh.\n",
        "Number of Apps Installed: Total apps available on the device.\n",
        "Data Usage: Daily mobile data consumption in megabytes.\n",
        "Age: Age of the user.\n",
        "Gender: Gender of the user (Male or Female).\n",
        "User Behavior Class: Classification of user behavior based on usage patterns (1 to 5).\n",
        "\n",
        "'''\n",
        "\n",
        "messages =[\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_message}\n",
        "\n",
        "]\n",
        "# Call the Azure OpenAI model\n",
        "response = await client.chat.completions.create(\n",
        "    model=azure_oai_deployment,\n",
        "    messages=messages,\n",
        "    temperature=0.7,\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "# Print the response to the console, if desired\n",
        "print(response)\n",
        "\n",
        "# Write the response to a file\n",
        "results_file = open(file=\"validation_tests.txt\", mode=\"w\", encoding=\"utf8\")\n",
        "results_file.write(response.choices[0].message.content)\n",
        "print(\"\\nResponse written to result/file.txt\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN1xEEapas7W",
        "outputId": "1957c00d-935e-4a80-858f-0308e2a50aec"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-AYtFCkRbn40DleDtj5cnI15w3I20F', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"To write the validation unit test cases for the provided dataset, you can use the Python `unittest` module. Here's an example of how you can structure the test cases:\\n\\n```python\\nimport unittest\\n\\nclass DatasetValidationTestCase(unittest.TestCase):\\n    def test_user_id(self):\\n        # Test that each user ID is unique\\n        # Your code here\\n        \\n    def test_device_model(self):\\n        # Test that device model is not empty\\n        # Your code here\\n        \\n    def test_operating_system(self):\\n        # Test that operating system is either 'iOS' or 'Android'\\n        # Your code here\\n        \\n    def test_app_usage_time(self):\\n        # Test that app usage time is greater than or equal to 0\\n        # Your code here\\n        \\n    def test_screen_on_time(self):\\n        # Test that screen on time is greater than or equal to 0\\n        # Your code here\\n        \\n    def test_battery_drain(self):\\n        # Test that battery drain is greater than or equal to 0\\n        # Your code here\\n        \\n    def test_number_of_apps_installed(self):\\n        # Test that number of apps installed is greater than or equal to 0\\n        # Your code here\\n        \\n    def test_data_usage(self):\\n        # Test that data usage is greater than or equal to 0\\n        # Your code here\\n        \\n    def test_age(self):\\n        # Test that age is greater than or equal to 0\\n        # Your code here\\n        \\n    def test_gender(self):\\n        # Test that gender is either 'Male' or 'Female'\\n        # Your code here\\n        \\n    def test_user_behavior_class(self):\\n        # Test that user behavior class is between 1 and 5\\n        # Your code here\\n\\nif __name__ == '__main__':\\n    unittest.main()\\n```\\n\\nIn each test case, you can write assertions to validate the specific criteria mentioned in the description. For example, in the `test_user_id` test case, you can check that each user ID is unique by comparing the IDs of all the samples in the dataset.\\n\\nYou can continue writing similar assertions for each test case to validate the dataset according to the provided description.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1732878874, model='gpt-35-turbo-16k', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=440, prompt_tokens=296, total_tokens=736, completion_tokens_details=None, prompt_tokens_details=None), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n",
            "\n",
            "Response written to result/file.txt\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0YD1pOb_ZrgV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}